# @package _global_

# to execute this experiment run:
# uv run train.py experiment=example

defaults:
  - override /data: fq
  - override /model: mambasp
  - override /callbacks: default
  - override /trainer: gpu

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

callbacks:
  early_stopping:
    patience: 10

tags: ["mambasp"]

seed: 12345

trainer:
  min_epochs: 10
  max_epochs: 100
  precision: "bf16-mixed"
  gradient_clip_val: 1.0

# mamba2 defaults:
#         d_state=128,
#         d_conv=4,
#         conv_init=None,
#         expand=2,
#         headdim=64,

# param_grid = {
#     'embedding_dim': [128, 256, 384],
#     'number_of_layers': [2, 3, 4],
#     'd_state': [64, 128, 256],
#     'dropout': [0.1, 0.2, 0.3],
#     'learning_rate': [1e-4, 3e-4, 1e-3]
# }

# d_model * expand / headdim = multiple of 8

model:
  net:
    number_of_layers: 2
    embedding_dim: 512
    d_state: 128
    d_conv: 4
    expand: 3
    headdim: 64
    dropout: 0
    number_of_classes: 2

  optimizer:
    _target_: torch.optim.AdamW
    _partial_: true
    lr: 0.0001
    weight_decay: 0.2

  scheduler:
    _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
    _partial_: true
    mode: min
    factor: 0.1
    patience: 4

data:
  tokenizer:
    model_max_length: 30000
    padding_side: "right"

  train_data_path: ${paths.root_dir}/data/train_data/p2_480000/train.parquet
  val_data_path: ${paths.root_dir}/data/train_data/p2_480000/validation.parquet
  test_data_path: ${paths.root_dir}/data/train_data/p2_480000/test.parquet
  batch_size: 64
  num_workers: 30
  pin_memory: False

logger:
  wandb:
    tags: ${tags}
    group: "mambasp"
  aim:
    experiment: "mambasp"

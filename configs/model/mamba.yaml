_target_: chimera.models.basic_module.ClassificationLit

net:
  _target_: chimera.models.cnn.MambaSequenceClassification
  vocab_size: 12
  number_of_classes: 2
  embedding_dim: 256
  num_filters: [256, 256, 256]
  kernel_sizes: [7, 7, 7]
  pool_sizes: [4, 4, 4]
  dropout: 0.1

optimizer:
  _target_: torch.optim.Adam
  _partial_: true
  lr: 0.001
  weight_decay: 0.0

scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  _partial_: true
  mode: min
  factor: 0.1
  patience: 10

criterion:
  _target_: torch.nn.CrossEntropyLoss

# compile model for faster training with pytorch 2.0
compile: false

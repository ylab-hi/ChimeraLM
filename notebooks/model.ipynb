{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53efce39-813a-494d-b2a1-c19edae5e0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07e207e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17dea6b7-8a23-43dc-9534-4be24a176ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_path = Path(\"/projects/b1171/ylk4626/project/Chimera\")\n",
    "project_path = Path(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a72af3-edf4-40aa-b320-db279a91bc4d",
   "metadata": {},
   "source": [
    "# Load Data from fq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0718aaa8-3648-4d58-a9c4-5f70d941b1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from needletail import parse_fastx_file\n",
    "\n",
    "\n",
    "def read_records(fastq):\n",
    "    if isinstance(fastq, Path):\n",
    "        fastq = fastq.as_posix()\n",
    "    return list(parse_fastx_file(fastq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f44bd8b-9d2a-4f53-9f17-e8e9b495ea69",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = project_path / \"data/train_data/80000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0b944cd-af2e-4dbc-8296-656b03eb8cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = dataset_folder / \"train.fq.target.fq.gz\"\n",
    "val_data_path = dataset_folder / \"val.fq.target.fq.gz\"\n",
    "test_data_path = dataset_folder / \"test.fq.target.fq.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9489b78e-c116-4623-8413-d2ef9045cc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_records(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7515e39a-e733-4a28-ba38-e6f5e4ea9e9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GGTAGGCGGGTTTCAGGGGCTCTTTGGTGAAGAGTTTTATGGCGTCAGCGAAGGGTTGTAGTAGCCCGTAGGGGCCTACAACGTTGGGGCCTTTGCGTAGTTGCTGTATCGCCTAGAATTTTTCGTTCGGTAAGCATTAGGAATGCCATTGCGATTAGAATGGGTACAATGAGGAGTAGGAGGTTGGCCATGGGTATGTTGTTAAGAAGAGGAATTGAACCTCTGACTGTAAAGTTTTAAGTTTTATGCGATTACCGGGCTCTGCCATCTTAACAAACCCCTGTTCTTGGGTGGGTGTGGGTATAATGCTAAGTTGAGATGATATCATTTACGGGGGAAGGCGCTTTGTGAAGTAGGCCTTATTTCTCTTGTCCTTTCGTACAGGGAGGAATTTGAAGTAGATAGAAACCGACCTGGATTACTCCGGTCTGAACTCAGATCACGTAGGACTTTAATGGTTGAACAAACGAACCTTTAATAGCGGCTGCACCATTGGGATGTCCTGATCCAACATCGAGGTCGTAAACCCTATTGTTGATATGGACTCTAGATAGGATTGCGCTGTTATCCCTAGGGTAACTTGTTCCGTTGGTCAAGTTATTGGATCAATTGAGTATAGTAGTTCGCTTTGACTGGTGAAGTCTTAGCATGTACTGCTCGGAGGTTGGGTTCTGCTCCGAGGTCGCCCCAACCGAAATTTTTAGATGCCGGTTTGGTCGTTTAGGACCTGTGGGTTTGTTAGGTACTGTTTGCATTAATAAATTAAAGCTCCATAGGGTCTTCTCGTCTTGCTGTGTCATGCCCGCCTCTTCACGGGCAGGTCAATTTCACTGGTTAAAAGTAAGAGACAGCTGAACCCTCGTGGAGCCATTCATACAGGTCCCTATTTAAGGAACAAGTGATTATGCTACCTTTGCACGGTTAGGGTACCAGGACCATTAAACATGTGTCACTGGGCAGGCGGTGCCTGATACTGGTGATGCTAGAGGTGATGTTTTTGGTAAACAGGCGGGGTGAGGTTTGCCAAGTACTCTTTTAACCTTTCCTTATGAGCATGCCTGTGTTGGGTTGACAGTGAGGGTAATAATGACTTGTTGGTTGATTGTAGATATTGGGGCTGTTAATTGTCAGTTCAGTGTTTAATCTGACCGCTTATGCGAAGAAGTGTTTTCATGTTACTTATACTAACATTAGTTCTTCTATAGGGTGATAGATTGGTCCAATATCTACAATCAACCAACAAGTCATTATTACCCTCACTGTCAACCCAACACAGGCATGCTCATAAGGAAAGGTTAAAAAAAAGTAAAAGGAACTCGGCAAATCTTACCCGCCTGTTTACCAAAAACATCACCTCTAGCATCACCAGTATTAGAGGCACCGCCTGCCCAGTGACACATGTTTAACGGCCGCGGTACCCTAACCGTGCAAAGGTAGCATAATCACTTGTTCCTTAAATAGGGACCTGTATGAATGGCTCCACGAGGGTTCAGCTGTCTCTACTTTTAACCAGTGAAATTGACCTGCCCGTGAAGAGGCGGGCATGACACAGCAAGACGAGAAGACCCTATGGAGCTTTAATTTATTAATGCAAACAGTACCTAACAAACCCACAGGTCCTAAACTACCAAACCTGCATTAAAAATTTCGGTTGGGGCGACCTCGGAGCAGAACCCAACCTCCGAGCAGTCATTTACTAAGACTTCACCAGTCAAAGCAAACTATTATACTCAATTGATCCAATAACTTGACCAACGGAACAAGTTACCCTAGGGATAACAGCGCAATCCTATTCTAGAGTCCATATCAACAATAGGGTTTACGACCTCAATGTTGGATCAGGACATCCCGATGGTGCAGCCGCTATTAAAGGTTCGTTTGTTCAACGATTAAAGTCCTACGTGATCTAGTAGACCGGAGTCATCCAGGTCGGTTTCTATCTTATTCAAATTCCTCCCTGTACGAAAGGACAAGAGAAATAAGGCCTACTTCACAAAGCGCCTTCCCGTAAATGATATCATCTCGATTTCAGCATATACCACACCCACCCAAGAACAGGGTTTGTTAAGATGGCAGAGCCCGGTAATCGCATAAACTTTAAAACTTTACAGTCAGAGGTTCAATTCCTCTTCTTAACAACATACCCATGGCCAACCTCCTACTCCTAGTATCCATTCTAATCGCAATGGCATTCCTAATTCTTACCGAACGAAAAATTCTAGGCTATATACAACTACGCAAAGGCCCCAACGTTCTAGCCCCCTACGGGCTACTACAACCCTTCGCTGACGCCATCAACTTCTTCACCAAACCCCCTAAAACCACCACCACGATGAT'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e740e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepbiop import fq\n",
    "\n",
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    "\n",
    "\n",
    "def parse_target(name):\n",
    "    \"\"\"Parse the target name to get the name and the target.\"\"\"\n",
    "    rid, target = name.split(\"|\")\n",
    "    return rid, int(target)\n",
    "\n",
    "\n",
    "def encode_qual(qual, offset=33):\n",
    "    \"\"\"Encode the quality score.\"\"\"\n",
    "    return list(fq.encode_qual(qual, offset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "be9c38bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting num_proc from 2 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2352d8d0a2ea460eba63edc69cf4e497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\n",
    "    \"train\": (dataset_folder / \"test.fq.target.fq.parquet\").as_posix(),\n",
    "}\n",
    "\n",
    "train_dataset = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files=data_files,\n",
    "    num_proc=2,\n",
    ").with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5cba98cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '349022f1-b68c-447e-bf8d-79d9e4c939ee|0',\n",
       " 'seq': 'GGTAGGCGGGTTTCAGGGGCTCTTTGGTGAAGAGTTTTATGGCGTCAGCGAAGGGTTGTAGTAGCCCGTAGGGGCCTACAACGTTGGGGCCTTTGCGTAGTTGCTGTATCGCCTAGAATTTTTCGTTCGGTAAGCATTAGGAATGCCATTGCGATTAGAATGGGTACAATGAGGAGTAGGAGGTTGGCCATGGGTATGTTGTTAAGAAGAGGAATTGAACCTCTGACTGTAAAGTTTTAAGTTTTATGCGATTACCGGGCTCTGCCATCTTAACAAACCCCTGTTCTTGGGTGGGTGTGGGTATAATGCTAAGTTGAGATGATATCATTTACGGGGGAAGGCGCTTTGTGAAGTAGGCCTTATTTCTCTTGTCCTTTCGTACAGGGAGGAATTTGAAGTAGATAGAAACCGACCTGGATTACTCCGGTCTGAACTCAGATCACGTAGGACTTTAATGGTTGAACAAACGAACCTTTAATAGCGGCTGCACCATTGGGATGTCCTGATCCAACATCGAGGTCGTAAACCCTATTGTTGATATGGACTCTAGATAGGATTGCGCTGTTATCCCTAGGGTAACTTGTTCCGTTGGTCAAGTTATTGGATCAATTGAGTATAGTAGTTCGCTTTGACTGGTGAAGTCTTAGCATGTACTGCTCGGAGGTTGGGTTCTGCTCCGAGGTCGCCCCAACCGAAATTTTTAGATGCCGGTTTGGTCGTTTAGGACCTGTGGGTTTGTTAGGTACTGTTTGCATTAATAAATTAAAGCTCCATAGGGTCTTCTCGTCTTGCTGTGTCATGCCCGCCTCTTCACGGGCAGGTCAATTTCACTGGTTAAAAGTAAGAGACAGCTGAACCCTCGTGGAGCCATTCATACAGGTCCCTATTTAAGGAACAAGTGATTATGCTACCTTTGCACGGTTAGGGTACCAGGACCATTAAACATGTGTCACTGGGCAGGCGGTGCCTGATACTGGTGATGCTAGAGGTGATGTTTTTGGTAAACAGGCGGGGTGAGGTTTGCCAAGTACTCTTTTAACCTTTCCTTATGAGCATGCCTGTGTTGGGTTGACAGTGAGGGTAATAATGACTTGTTGGTTGATTGTAGATATTGGGGCTGTTAATTGTCAGTTCAGTGTTTAATCTGACCGCTTATGCGAAGAAGTGTTTTCATGTTACTTATACTAACATTAGTTCTTCTATAGGGTGATAGATTGGTCCAATATCTACAATCAACCAACAAGTCATTATTACCCTCACTGTCAACCCAACACAGGCATGCTCATAAGGAAAGGTTAAAAAAAAGTAAAAGGAACTCGGCAAATCTTACCCGCCTGTTTACCAAAAACATCACCTCTAGCATCACCAGTATTAGAGGCACCGCCTGCCCAGTGACACATGTTTAACGGCCGCGGTACCCTAACCGTGCAAAGGTAGCATAATCACTTGTTCCTTAAATAGGGACCTGTATGAATGGCTCCACGAGGGTTCAGCTGTCTCTACTTTTAACCAGTGAAATTGACCTGCCCGTGAAGAGGCGGGCATGACACAGCAAGACGAGAAGACCCTATGGAGCTTTAATTTATTAATGCAAACAGTACCTAACAAACCCACAGGTCCTAAACTACCAAACCTGCATTAAAAATTTCGGTTGGGGCGACCTCGGAGCAGAACCCAACCTCCGAGCAGTCATTTACTAAGACTTCACCAGTCAAAGCAAACTATTATACTCAATTGATCCAATAACTTGACCAACGGAACAAGTTACCCTAGGGATAACAGCGCAATCCTATTCTAGAGTCCATATCAACAATAGGGTTTACGACCTCAATGTTGGATCAGGACATCCCGATGGTGCAGCCGCTATTAAAGGTTCGTTTGTTCAACGATTAAAGTCCTACGTGATCTAGTAGACCGGAGTCATCCAGGTCGGTTTCTATCTTATTCAAATTCCTCCCTGTACGAAAGGACAAGAGAAATAAGGCCTACTTCACAAAGCGCCTTCCCGTAAATGATATCATCTCGATTTCAGCATATACCACACCCACCCAAGAACAGGGTTTGTTAAGATGGCAGAGCCCGGTAATCGCATAAACTTTAAAACTTTACAGTCAGAGGTTCAATTCCTCTTCTTAACAACATACCCATGGCCAACCTCCTACTCCTAGTATCCATTCTAATCGCAATGGCATTCCTAATTCTTACCGAACGAAAAATTCTAGGCTATATACAACTACGCAAAGGCCCCAACGTTCTAGCCCCCTACGGGCTACTACAACCCTTCGCTGACGCCATCAACTTCTTCACCAAACCCCCTAAAACCACCACCACGATGAT',\n",
       " 'qual': tensor([6, 7, 7,  ..., 3, 3, 3])}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f9717383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "\n",
    "class Tokenizer(PreTrainedTokenizer):\n",
    "    model_input_names = [\"input_ids\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_max_length: int,\n",
    "        bos_token=\"[BOS]\",\n",
    "        eos_token=\"[SEP]\",\n",
    "        sep_token=\"[SEP]\",\n",
    "        cls_token=\"[CLS]\",\n",
    "        pad_token=\"[PAD]\",\n",
    "        mask_token=\"[MASK]\",\n",
    "        unk_token=\"[UNK]\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Character tokenizer for Hugging Face transformers.\n",
    "\n",
    "        Args:\n",
    "            characters (Sequence[str]): List of desired characters. Any character which\n",
    "                is not included in this list will be replaced by a special token called\n",
    "                [UNK] with id=6. Following are list of all of the special tokens with\n",
    "                their corresponding ids:\n",
    "                    \"[CLS]\": 0\n",
    "                    \"[SEP]\": 1\n",
    "                    \"[BOS]\": 2\n",
    "                    \"[MASK]\": 3\n",
    "                    \"[PAD]\": 4\n",
    "                    \"[RESERVED]\": 5\n",
    "                    \"[UNK]\": 6\n",
    "                an id (starting at 7) will be assigned to each character.\n",
    "            model_max_length (int): Model maximum sequence length.\n",
    "        \"\"\"\n",
    "        self.characters = (\"A\", \"C\", \"G\", \"T\", \"N\")\n",
    "        self.model_max_length = model_max_length\n",
    "\n",
    "        self._vocab_str_to_int = {\n",
    "            \"[CLS]\": 0,\n",
    "            \"[SEP]\": 1,\n",
    "            \"[BOS]\": 2,\n",
    "            \"[MASK]\": 3,\n",
    "            \"[PAD]\": 4,\n",
    "            \"[RESERVED]\": 5,\n",
    "            \"[UNK]\": 6,\n",
    "            **{ch: i + 7 for i, ch in enumerate(self.characters)},\n",
    "        }\n",
    "        self._vocab_int_to_str = {v: k for k, v in self._vocab_str_to_int.items()}\n",
    "        add_prefix_space = kwargs.pop(\"add_prefix_space\", False)\n",
    "        padding_side = kwargs.pop(\"padding_side\", \"right\")\n",
    "\n",
    "        super().__init__(\n",
    "            bos_token=bos_token,\n",
    "            eos_token=eos_token,\n",
    "            sep_token=sep_token,\n",
    "            cls_token=cls_token,\n",
    "            pad_token=pad_token,\n",
    "            mask_token=mask_token,\n",
    "            unk_token=unk_token,\n",
    "            add_prefix_space=add_prefix_space,\n",
    "            model_max_length=model_max_length,\n",
    "            padding_side=padding_side,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self._vocab_str_to_int)\n",
    "\n",
    "    def _tokenize(self, text: str) -> list[str]:\n",
    "        return list(text)\n",
    "\n",
    "    def _convert_token_to_id(self, token: str) -> int:\n",
    "        return self._vocab_str_to_int.get(token, self._vocab_str_to_int[\"[UNK]\"])\n",
    "\n",
    "    def _convert_id_to_token(self, index: int) -> str:\n",
    "        return self._vocab_int_to_str[index]\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        return \"\".join(tokens)\n",
    "\n",
    "    def get_special_tokens_mask(\n",
    "        self,\n",
    "        token_ids_0: list[int],\n",
    "        token_ids_1: list[int] | None = None,\n",
    "        *,\n",
    "        already_has_special_tokens: bool = False,\n",
    "    ) -> list[int]:\n",
    "        if already_has_special_tokens:\n",
    "            return super().get_special_tokens_mask(\n",
    "                token_ids_0=token_ids_0,\n",
    "                token_ids_1=token_ids_1,\n",
    "                already_has_special_tokens=True,\n",
    "            )\n",
    "\n",
    "        result = ([0] * len(token_ids_0)) + [1]\n",
    "        if token_ids_1 is not None:\n",
    "            result += ([0] * len(token_ids_1)) + [1]\n",
    "        return result\n",
    "\n",
    "    def build_inputs_with_special_tokens(\n",
    "        self, token_ids_0: list[int], token_ids_1: list[int] | None = None\n",
    "    ) -> list[int]:\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        result = cls + token_ids_0 + sep\n",
    "        if token_ids_1 is not None:\n",
    "            result += token_ids_1 + sep\n",
    "        return result\n",
    "\n",
    "    def get_vocab(self) -> dict[str, int]:\n",
    "        return self._vocab_str_to_int\n",
    "\n",
    "    def decode(self, token_ids, skip_special_tokens=False, **kwargs):\n",
    "        \"\"\"Decode ids back to sequence string.\"\"\"\n",
    "        if isinstance(token_ids, dict):\n",
    "            token_ids = token_ids[\"input_ids\"]\n",
    "\n",
    "        if isinstance(token_ids, torch.Tensor):\n",
    "            token_ids = token_ids.tolist()\n",
    "\n",
    "        if isinstance(token_ids, list) and isinstance(token_ids[0], list):\n",
    "            token_ids = token_ids[0]  # Take first sequence if batch\n",
    "\n",
    "        tokens = [self._convert_id_to_token(id) for id in token_ids[\"input_ids\"]]\n",
    "        if skip_special_tokens:\n",
    "            tokens = [token for token in tokens if token not in self.all_special_tokens]\n",
    "\n",
    "        return self.convert_tokens_to_string(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f2e10b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(model_max_length=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bd1ff025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 9, 9, 10, 7, 9, 9, 8, 9, 9, 9, 10, 10, 10, 8, 7, 9, 9, 9, 9, 8, 10, 8, 10, 10, 10, 9, 9, 10, 9, 7, 7, 9, 7, 9, 10, 10, 10, 10, 7, 10, 9, 9, 8, 9, 10, 8, 7, 9, 8, 9, 7, 7, 9, 9, 9, 10, 10, 9, 10, 7, 9, 10, 7, 9, 8, 8, 8, 9, 10, 7, 9, 9, 9, 9, 8, 8, 10, 7, 8, 7, 7, 8, 9, 10, 10, 9, 9, 9, 9, 8, 8, 10, 10, 10, 9, 8, 9, 10, 7, 9, 10, 10, 9, 8, 10, 9, 10, 7, 10, 8, 9, 8, 8, 10, 7, 9, 7, 7, 10, 10, 10, 10, 10, 8, 9, 10, 10, 8, 9, 9, 10, 7, 7, 9, 8, 7, 10, 10, 7, 9, 9, 7, 7, 10, 9, 8, 8, 7, 10, 10, 9, 8, 9, 7, 10, 10, 7, 9, 7, 7, 10, 9, 9, 9, 10, 7, 8, 7, 7, 10, 9, 7, 9, 9, 7, 9, 10, 7, 9, 9, 7, 9, 9, 10, 10, 9, 9, 8, 8, 7, 10, 9, 9, 9, 10, 7, 10, 9, 10, 10, 9, 10, 10, 7, 7, 9, 7, 7, 9, 7, 9, 9, 7, 7, 10, 10, 9, 7, 7, 8, 8, 10, 8, 10, 9, 7, 8, 10, 9, 10, 7, 7, 7, 9, 10, 10, 10, 10, 7, 7, 9, 10, 10, 10, 10, 7, 10, 9, 8, 9, 7, 10, 10, 7, 8, 8, 9, 9, 9, 8, 10, 8, 10, 9, 8, 8, 7, 10, 8, 10, 10, 7, 7, 8, 7, 7, 7, 8, 8, 8, 8, 10, 9, 10, 10, 8, 10, 10, 9, 9, 9, 10, 9, 9, 9, 10, 9, 10, 9, 9, 9, 10, 7, 10, 7, 7, 10, 9, 8, 10, 7, 7, 9, 10, 10, 9, 7, 9, 7, 10, 9, 7, 10, 7, 10, 8, 7, 10, 10, 10, 7, 8, 9, 9, 9, 9, 9, 7, 7, 9, 9, 8, 9, 8, 10, 10, 10, 9, 10, 9, 7, 7, 9, 10, 7, 9, 9, 8, 8, 10, 10, 7, 10, 10, 10, 8, 10, 8, 10, 10, 9, 10, 8, 8, 10, 10, 10, 8, 9, 10, 7, 8, 7, 9, 9, 9, 7, 9, 9, 7, 7, 10, 10, 10, 9, 7, 7, 9, 10, 7, 9, 7, 10, 7, 9, 7, 7, 7, 8, 8, 9, 7, 8, 8, 10, 9, 9, 7, 10, 10, 7, 8, 10, 8, 8, 9, 9, 10, 8, 10, 9, 7, 7, 8, 10, 8, 7, 9, 7, 10, 8, 7, 8, 9, 10, 7, 9, 9, 7, 8, 10, 10, 10, 7, 7, 10, 9, 9, 10, 10, 9, 7, 7, 8, 7, 7, 7, 8, 9, 7, 7, 8, 8, 10, 10, 10, 7, 7, 10, 7, 9, 8, 9, 9, 8, 10, 9, 8, 7, 8, 8, 7, 10, 10, 9, 9, 9, 7, 10, 9, 10, 8, 8, 10, 9, 7, 10, 8, 8, 7, 7, 8, 7, 10, 8, 9, 7, 9, 9, 10, 8, 9, 10, 7, 7, 7, 8, 8, 8, 10, 7, 10, 10, 9, 10, 10, 9, 7, 10, 7, 10, 9, 9, 7, 8, 10, 8, 10, 7, 9, 7, 10, 7, 9, 9, 7, 10, 10, 9, 8, 9, 8, 10, 9, 10, 10, 7, 10, 8, 8, 8, 10, 7, 9, 9, 9, 10, 7, 7, 8, 10, 10, 9, 10, 10, 8, 8, 9, 10, 10, 9, 9, 10, 8, 7, 7, 9, 10, 10, 7, 10, 10, 9, 9, 7, 10, 8, 7, 7, 10, 10, 9, 7, 9, 10, 7, 10, 7, 9, 10, 7, 9, 10, 10, 8, 9, 8, 10, 10, 10, 9, 7, 8, 10, 9, 9, 10, 9, 7, 7, 9, 10, 8, 10, 10, 7, 9, 8, 7, 10, 9, 10, 7, 8, 10, 9, 8, 10, 8, 9, 9, 7, 9, 9, 10, 10, 9, 9, 9, 10, 10, 8, 10, 9, 8, 10, 8, 8, 9, 7, 9, 9, 10, 8, 9, 8, 8, 8, 8, 7, 7, 8, 8, 9, 7, 7, 7, 10, 10, 10, 10, 10, 7, 9, 7, 10, 9, 8, 8, 9, 9, 10, 10, 10, 9, 9, 10, 8, 9, 10, 10, 10, 7, 9, 9, 7, 8, 8, 10, 9, 10, 9, 9, 9, 10, 10, 10, 9, 10, 10, 7, 9, 9, 10, 7, 8, 10, 9, 10, 10, 10, 9, 8, 7, 10, 10, 7, 7, 10, 7, 7, 7, 10, 10, 7, 7, 7, 9, 8, 10, 8, 8, 7, 10, 7, 9, 9, 9, 10, 8, 10, 10, 8, 10, 8, 9, 10, 8, 10, 10, 9, 8, 10, 9, 10, 9, 10, 8, 7, 10, 9, 8, 8, 8, 9, 8, 8, 10, 8, 10, 10, 8, 7, 8, 9, 9, 9, 8, 7, 9, 9, 10, 8, 7, 7, 10, 10, 10, 8, 7, 8, 10, 9, 9, 10, 10, 7, 7, 7, 7, 9, 10, 7, 7, 9, 7, 9, 7, 8, 7, 9, 8, 10, 9, 7, 7, 8, 8, 8, 10, 8, 9, 10, 9, 9, 7, 9, 8, 8, 7, 10, 10, 8, 7, 10, 7, 8, 7, 9, 9, 10, 8, 8, 8, 10, 7, 10, 10, 10, 7, 7, 9, 9, 7, 7, 8, 7, 7, 9, 10, 9, 7, 10, 10, 7, 10, 9, 8, 10, 7, 8, 8, 10, 10, 10, 9, 8, 7, 8, 9, 9, 10, 10, 7, 9, 9, 9, 10, 7, 8, 8, 7, 9, 9, 7, 8, 8, 7, 10, 10, 7, 7, 7, 8, 7, 10, 9, 10, 9, 10, 8, 7, 8, 10, 9, 9, 9, 8, 7, 9, 9, 8, 9, 9, 10, 9, 8, 8, 10, 9, 7, 10, 7, 8, 10, 9, 9, 10, 9, 7, 10, 9, 8, 10, 7, 9, 7, 9, 9, 10, 9, 7, 10, 9, 10, 10, 10, 10, 10, 9, 9, 10, 7, 7, 7, 8, 7, 9, 9, 8, 9, 9, 9, 9, 10, 9, 7, 9, 9, 10, 10, 10, 9, 8, 8, 7, 7, 9, 10, 7, 8, 10, 8, 10, 10, 10, 10, 7, 7, 8, 8, 10, 10, 10, 8, 8, 10, 10, 7, 10, 9, 7, 9, 8, 7, 10, 9, 8, 8, 10, 9, 10, 9, 10, 10, 9, 9, 9, 10, 10, 9, 7, 8, 7, 9, 10, 9, 7, 9, 9, 9, 10, 7, 7, 10, 7, 7, 10, 9, 7, 8, 10, 10, 9, 10, 10, 9, 9, 10, 10, 9, 7, 10, 10, 9, 10, 7, 9, 7, 10, 7, 10, 10, 9, 9, 9, 9, 8, 10, 9, 10, 10, 7, 7, 10, 10, 9, 10, 8, 7, 9, 10, 10, 8, 7, 9, 10, 9, 10, 10, 10, 7, 7, 10, 8, 10, 9, 7, 8, 8, 9, 8, 10, 10, 7, 10, 9, 8, 9, 7, 7, 9, 7, 7, 9, 10, 9, 10, 10, 10, 10, 8, 7, 10, 9, 10, 10, 7, 8, 10, 10, 7, 10, 7, 8, 10, 7, 7, 8, 7, 10, 10, 7, 9, 10, 10, 8, 10, 10, 8, 10, 7, 10, 7, 9, 9, 9, 10, 9, 7, 10, 7, 9, 7, 10, 10, 9, 9, 10, 8, 8, 7, 7, 10, 7, 10, 8, 10, 7, 8, 7, 7, 10, 8, 7, 7, 8, 8, 7, 7, 8, 7, 7, 9, 10, 8, 7, 10, 10, 7, 10, 10, 7, 8, 8, 8, 10, 8, 7, 8, 10, 9, 10, 8, 7, 7, 8, 8, 8, 7, 7, 8, 7, 8, 7, 9, 9, 8, 7, 10, 9, 8, 10, 8, 7, 10, 7, 7, 9, 9, 7, 7, 7, 9, 9, 10, 10, 7, 7, 7, 7, 7, 7, 7, 7, 9, 10, 7, 7, 7, 7, 9, 9, 7, 7, 8, 10, 8, 9, 9, 8, 7, 7, 7, 10, 8, 10, 10, 7, 8, 8, 8, 9, 8, 8, 10, 9, 10, 10, 10, 7, 8, 8, 7, 7, 7, 7, 7, 8, 7, 10, 8, 7, 8, 8, 10, 8, 10, 7, 9, 8, 7, 10, 8, 7, 8, 8, 7, 9, 10, 7, 10, 10, 7, 9, 7, 9, 9, 8, 7, 8, 8, 9, 8, 8, 10, 9, 8, 8, 8, 7, 9, 10, 9, 7, 8, 7, 8, 7, 10, 9, 10, 10, 10, 7, 7, 8, 9, 9, 8, 8, 9, 8, 9, 9, 10, 7, 8, 8, 8, 10, 7, 7, 8, 8, 9, 10, 9, 8, 7, 7, 7, 9, 9, 10, 7, 9, 8, 7, 10, 7, 7, 10, 8, 7, 8, 10, 10, 9, 10, 10, 8, 8, 10, 10, 7, 7, 7, 10, 7, 9, 9, 9, 7, 8, 8, 10, 9, 10, 7, 10, 9, 7, 7, 10, 9, 9, 8, 10, 8, 8, 7, 8, 9, 7, 9, 9, 9, 10, 10, 8, 7, 9, 8, 10, 9, 10, 8, 10, 8, 10, 7, 8, 10, 10, 10, 10, 7, 7, 8, 8, 7, 9, 10, 9, 7, 7, 7, 10, 10, 9, 7, 8, 8, 10, 9, 8, 8, 8, 9, 10, 9, 7, 7, 9, 7, 9, 9, 8, 9, 9, 9, 8, 7, 10, 9, 7, 8, 7, 8, 7, 9, 8, 7, 7, 9, 7, 8, 9, 7, 9, 7, 7, 9, 7, 8, 8, 8, 10, 7, 10, 9, 9, 7, 9, 8, 10, 10, 10, 7, 7, 10, 10, 10, 7, 10, 10, 7, 7, 10, 9, 8, 7, 7, 7, 8, 7, 9, 10, 7, 8, 8, 10, 7, 7, 8, 7, 7, 7, 8, 8, 8, 7, 8, 7, 9, 9, 10, 8, 8, 10, 7, 7, 7, 8, 10, 7, 8, 8, 7, 7, 7, 8, 8, 10, 9, 8, 7, 10, 10, 7, 7, 7, 7, 7, 10, 10, 10, 8, 9, 9, 10, 10, 9, 9, 9, 9, 8, 9, 7, 8, 8, 10, 8, 9, 9, 7, 9, 8, 7, 9, 7, 7, 8, 8, 8, 7, 7, 8, 8, 10, 8, 8, 9, 7, 9, 8, 7, 9, 10, 8, 7, 10, 10, 10, 7, 8, 10, 7, 7, 9, 7, 8, 10, 10, 8, 7, 8, 8, 7, 9, 10, 8, 7, 7, 7, 9, 8, 7, 7, 7, 8, 10, 7, 10, 10, 7, 10, 7, 8, 10, 8, 7, 7, 10, 10, 9, 7, 10, 8, 8, 7, 7, 10, 7, 7, 8, 10, 10, 9, 7, 8, 8, 7, 7, 8, 9, 9, 7, 7, 8, 7, 7, 9, 10, 10, 7, 8, 8, 8, 10, 7, 9, 9, 9, 7, 10, 7, 7, 8, 7, 9, 8, 9, 8, 7, 7, 10, 8, 8, 10, 7, 10, 10, 8, 10, 7, 9, 7, 9, 10, 8, 8, 7, 10, 7, 10, 8, 7, 7, 8, 7, 7, 10, 7, 9, 9, 9, 10, 10, 10, 7, 8, 9, 7, 8, 8, 10, 8, 7, 7, 10, 9, 10, 10, 9, 9, 7, 10, 8, 7, 9, 9, 7, 8, 7, 10, 8, 8, 8, 9, 7, 10, 9, 9, 10, 9, 8, 7, 9, 8, 8, 9, 8, 10, 7, 10, 10, 7, 7, 7, 9, 9, 10, 10, 8, 9, 10, 10, 10, 9, 10, 10, 8, 7, 7, 8, 9, 7, 10, 10, 7, 7, 7, 9, 10, 8, 8, 10, 7, 8, 9, 10, 9, 7, 10, 8, 10, 7, 9, 10, 7, 9, 7, 8, 8, 9, 9, 7, 9, 10, 8, 7, 10, 8, 8, 7, 9, 9, 10, 8, 9, 9, 10, 10, 10, 8, 10, 7, 10, 8, 10, 10, 7, 10, 10, 8, 7, 7, 7, 10, 10, 8, 8, 10, 8, 8, 8, 10, 9, 10, 7, 8, 9, 7, 7, 7, 9, 9, 7, 8, 7, 7, 9, 7, 9, 7, 7, 7, 10, 7, 7, 9, 9, 8, 8, 10, 7, 8, 10, 10, 8, 7, 8, 7, 7, 7, 9, 8, 9, 8, 8, 10, 10, 8, 8, 8, 9, 10, 7, 7, 7, 10, 9, 7, 10, 7, 10, 8, 7, 10, 8, 10, 8, 9, 7, 10, 10, 10, 8, 7, 9, 8, 7, 10, 7, 10, 7, 8, 8, 7, 8, 7, 8, 8, 8, 7, 8, 8, 8, 7, 7, 9, 7, 7, 8, 7, 9, 9, 9, 10, 10, 10, 9, 10, 10, 7, 7, 9, 7, 10, 9, 9, 8, 7, 9, 7, 9, 8, 8, 8, 9, 9, 10, 7, 7, 10, 8, 9, 8, 7, 10, 7, 7, 7, 8, 10, 10, 10, 7, 7, 7, 7, 8, 10, 10, 10, 7, 8, 7, 9, 10, 8, 7, 9, 7, 9, 9, 10, 10, 8, 7, 7, 10, 10, 8, 8, 10, 8, 10, 10, 8, 10, 10, 7, 7, 8, 7, 7, 8, 7, 10, 7, 8, 8, 8, 7, 10, 9, 9, 8, 8, 7, 7, 8, 8, 10, 8, 8, 10, 7, 8, 10, 8, 8, 10, 7, 9, 10, 7, 10, 8, 8, 7, 10, 10, 8, 10, 7, 7, 10, 8, 9, 8, 7, 7, 10, 9, 9, 8, 7, 10, 10, 8, 8, 10, 7, 7, 10, 10, 8, 10, 10, 7, 8, 8, 9, 7, 7, 8, 9, 7, 7, 7, 7, 7, 10, 10, 8, 10, 7, 9, 9, 8, 10, 7, 10, 7, 10, 7, 8, 7, 7, 8, 10, 7, 8, 9, 8, 7, 7, 7, 9, 9, 8, 8, 8, 8, 7, 7, 8, 9, 10, 10, 8, 10, 7, 9, 8, 8, 8, 8, 8, 10, 7, 8, 9, 9, 9, 8, 10, 7, 8, 10, 7, 8, 7, 7, 8, 8, 8, 10, 10, 8, 9, 8, 10, 9, 7, 8, 9, 8, 8, 7, 10, 8, 7, 7, 8, 10, 10, 8, 10, 10, 8, 7, 8, 8, 7, 7, 7, 8, 8, 8, 8, 8, 10, 7, 7, 7, 7, 8, 8, 7, 8, 8, 7, 8, 8, 7, 8, 9, 7, 10, 9, 7, 10, 1]}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(train_dataset[\"train\"][0][\"seq\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2cefb71b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]GGTAGGCGGGTTTCAGGGGCTCTTTGGTGAAGAGTTTTATGGCGTCAGCGAAGGGTTGTAGTAGCCCGTAGGGGCCTACAACGTTGGGGCCTTTGCGTAGTTGCTGTATCGCCTAGAATTTTTCGTTCGGTAAGCATTAGGAATGCCATTGCGATTAGAATGGGTACAATGAGGAGTAGGAGGTTGGCCATGGGTATGTTGTTAAGAAGAGGAATTGAACCTCTGACTGTAAAGTTTTAAGTTTTATGCGATTACCGGGCTCTGCCATCTTAACAAACCCCTGTTCTTGGGTGGGTGTGGGTATAATGCTAAGTTGAGATGATATCATTTACGGGGGAAGGCGCTTTGTGAAGTAGGCCTTATTTCTCTTGTCCTTTCGTACAGGGAGGAATTTGAAGTAGATAGAAACCGACCTGGATTACTCCGGTCTGAACTCAGATCACGTAGGACTTTAATGGTTGAACAAACGAACCTTTAATAGCGGCTGCACCATTGGGATGTCCTGATCCAACATCGAGGTCGTAAACCCTATTGTTGATATGGACTCTAGATAGGATTGCGCTGTTATCCCTAGGGTAACTTGTTCCGTTGGTCAAGTTATTGGATCAATTGAGTATAGTAGTTCGCTTTGACTGGTGAAGTCTTAGCATGTACTGCTCGGAGGTTGGGTTCTGCTCCGAGGTCGCCCCAACCGAAATTTTTAGATGCCGGTTTGGTCGTTTAGGACCTGTGGGTTTGTTAGGTACTGTTTGCATTAATAAATTAAAGCTCCATAGGGTCTTCTCGTCTTGCTGTGTCATGCCCGCCTCTTCACGGGCAGGTCAATTTCACTGGTTAAAAGTAAGAGACAGCTGAACCCTCGTGGAGCCATTCATACAGGTCCCTATTTAAGGAACAAGTGATTATGCTACCTTTGCACGGTTAGGGTACCAGGACCATTAAACATGTGTCACTGGGCAGGCGGTGCCTGATACTGGTGATGCTAGAGGTGATGTTTTTGGTAAACAGGCGGGGTGAGGTTTGCCAAGTACTCTTTTAACCTTTCCTTATGAGCATGCCTGTGTTGGGTTGACAGTGAGGGTAATAATGACTTGTTGGTTGATTGTAGATATTGGGGCTGTTAATTGTCAGTTCAGTGTTTAATCTGACCGCTTATGCGAAGAAGTGTTTTCATGTTACTTATACTAACATTAGTTCTTCTATAGGGTGATAGATTGGTCCAATATCTACAATCAACCAACAAGTCATTATTACCCTCACTGTCAACCCAACACAGGCATGCTCATAAGGAAAGGTTAAAAAAAAGTAAAAGGAACTCGGCAAATCTTACCCGCCTGTTTACCAAAAACATCACCTCTAGCATCACCAGTATTAGAGGCACCGCCTGCCCAGTGACACATGTTTAACGGCCGCGGTACCCTAACCGTGCAAAGGTAGCATAATCACTTGTTCCTTAAATAGGGACCTGTATGAATGGCTCCACGAGGGTTCAGCTGTCTCTACTTTTAACCAGTGAAATTGACCTGCCCGTGAAGAGGCGGGCATGACACAGCAAGACGAGAAGACCCTATGGAGCTTTAATTTATTAATGCAAACAGTACCTAACAAACCCACAGGTCCTAAACTACCAAACCTGCATTAAAAATTTCGGTTGGGGCGACCTCGGAGCAGAACCCAACCTCCGAGCAGTCATTTACTAAGACTTCACCAGTCAAAGCAAACTATTATACTCAATTGATCCAATAACTTGACCAACGGAACAAGTTACCCTAGGGATAACAGCGCAATCCTATTCTAGAGTCCATATCAACAATAGGGTTTACGACCTCAATGTTGGATCAGGACATCCCGATGGTGCAGCCGCTATTAAAGGTTCGTTTGTTCAACGATTAAAGTCCTACGTGATCTAGTAGACCGGAGTCATCCAGGTCGGTTTCTATCTTATTCAAATTCCTCCCTGTACGAAAGGACAAGAGAAATAAGGCCTACTTCACAAAGCGCCTTCCCGTAAATGATATCATCTCGATTTCAGCATATACCACACCCACCCAAGAACAGGGTTTGTTAAGATGGCAGAGCCCGGTAATCGCATAAACTTTAAAACTTTACAGTCAGAGGTTCAATTCCTCTTCTTAACAACATACCCATGGCCAACCTCCTACTCCTAGTATCCATTCTAATCGCAATGGCATTCCTAATTCTTACCGAACGAAAAATTCTAGGCTATATACAACTACGCAAAGGCCCCAACGTTCTAGCCCCCTACGGGCTACTACAACCCTTCGCTGACGCCATCAACTTCTTCACCAAACCCCCTAAAACCACCACCACGATGAT[SEP]'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer(train_dataset[\"train\"][0][\"seq\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "22123fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GGTAGGCGGGTTTCAGGGGCTCTTTGGTGAAGAGTTTTATGGCGTCAGCGAAGGGTTGTAGTAGCCCGTAGGGGCCTACAACGTTGGGGCCTTTGCGTAGTTGCTGTATCGCCTAGAATTTTTCGTTCGGTAAGCATTAGGAATGCCATTGCGATTAGAATGGGTACAATGAGGAGTAGGAGGTTGGCCATGGGTATGTTGTTAAGAAGAGGAATTGAACCTCTGACTGTAAAGTTTTAAGTTTTATGCGATTACCGGGCTCTGCCATCTTAACAAACCCCTGTTCTTGGGTGGGTGTGGGTATAATGCTAAGTTGAGATGATATCATTTACGGGGGAAGGCGCTTTGTGAAGTAGGCCTTATTTCTCTTGTCCTTTCGTACAGGGAGGAATTTGAAGTAGATAGAAACCGACCTGGATTACTCCGGTCTGAACTCAGATCACGTAGGACTTTAATGGTTGAACAAACGAACCTTTAATAGCGGCTGCACCATTGGGATGTCCTGATCCAACATCGAGGTCGTAAACCCTATTGTTGATATGGACTCTAGATAGGATTGCGCTGTTATCCCTAGGGTAACTTGTTCCGTTGGTCAAGTTATTGGATCAATTGAGTATAGTAGTTCGCTTTGACTGGTGAAGTCTTAGCATGTACTGCTCGGAGGTTGGGTTCTGCTCCGAGGTCGCCCCAACCGAAATTTTTAGATGCCGGTTTGGTCGTTTAGGACCTGTGGGTTTGTTAGGTACTGTTTGCATTAATAAATTAAAGCTCCATAGGGTCTTCTCGTCTTGCTGTGTCATGCCCGCCTCTTCACGGGCAGGTCAATTTCACTGGTTAAAAGTAAGAGACAGCTGAACCCTCGTGGAGCCATTCATACAGGTCCCTATTTAAGGAACAAGTGATTATGCTACCTTTGCACGGTTAGGGTACCAGGACCATTAAACATGTGTCACTGGGCAGGCGGTGCCTGATACTGGTGATGCTAGAGGTGATGTTTTTGGTAAACAGGCGGGGTGAGGTTTGCCAAGTACTCTTTTAACCTTTCCTTATGAGCATGCCTGTGTTGGGTTGACAGTGAGGGTAATAATGACTTGTTGGTTGATTGTAGATATTGGGGCTGTTAATTGTCAGTTCAGTGTTTAATCTGACCGCTTATGCGAAGAAGTGTTTTCATGTTACTTATACTAACATTAGTTCTTCTATAGGGTGATAGATTGGTCCAATATCTACAATCAACCAACAAGTCATTATTACCCTCACTGTCAACCCAACACAGGCATGCTCATAAGGAAAGGTTAAAAAAAAGTAAAAGGAACTCGGCAAATCTTACCCGCCTGTTTACCAAAAACATCACCTCTAGCATCACCAGTATTAGAGGCACCGCCTGCCCAGTGACACATGTTTAACGGCCGCGGTACCCTAACCGTGCAAAGGTAGCATAATCACTTGTTCCTTAAATAGGGACCTGTATGAATGGCTCCACGAGGGTTCAGCTGTCTCTACTTTTAACCAGTGAAATTGACCTGCCCGTGAAGAGGCGGGCATGACACAGCAAGACGAGAAGACCCTATGGAGCTTTAATTTATTAATGCAAACAGTACCTAACAAACCCACAGGTCCTAAACTACCAAACCTGCATTAAAAATTTCGGTTGGGGCGACCTCGGAGCAGAACCCAACCTCCGAGCAGTCATTTACTAAGACTTCACCAGTCAAAGCAAACTATTATACTCAATTGATCCAATAACTTGACCAACGGAACAAGTTACCCTAGGGATAACAGCGCAATCCTATTCTAGAGTCCATATCAACAATAGGGTTTACGACCTCAATGTTGGATCAGGACATCCCGATGGTGCAGCCGCTATTAAAGGTTCGTTTGTTCAACGATTAAAGTCCTACGTGATCTAGTAGACCGGAGTCATCCAGGTCGGTTTCTATCTTATTCAAATTCCTCCCTGTACGAAAGGACAAGAGAAATAAGGCCTACTTCACAAAGCGCCTTCCCGTAAATGATATCATCTCGATTTCAGCATATACCACACCCACCCAAGAACAGGGTTTGTTAAGATGGCAGAGCCCGGTAATCGCATAAACTTTAAAACTTTACAGTCAGAGGTTCAATTCCTCTTCTTAACAACATACCCATGGCCAACCTCCTACTCCTAGTATCCATTCTAATCGCAATGGCATTCCTAATTCTTACCGAACGAAAAATTCTAGGCTATATACAACTACGCAAAGGCCCCAACGTTCTAGCCCCCTACGGGCTACTACAACCCTTCGCTGACGCCATCAACTTCTTCACCAAACCCCCTAAAACCACCACCACGATGAT'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"train\"][0][\"seq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "862a9546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[BOS]', '[SEP]', '[UNK]', '[PAD]', '[CLS]', '[MASK]']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ec986a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 7, 7,  ..., 3, 3, 3])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"train\"][0][\"qual\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9861d71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc4a80d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4171a9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels_and_quals(data, tokenizer, max_length, pad_qual=0):\n",
    "    tokenized_inputs = tokenizer(data[\"seq\"], max_length=max_length, truncation=True, padding=True)\n",
    "    if len(data[\"seq\"]) >= max_length:\n",
    "        quals = torch.cat((data[\"qual\"][: max_length - 1], torch.tensor([pad_qual]))).float()\n",
    "        normalized_quals = torch.nn.functional.normalize(quals, dim=0)\n",
    "    else:\n",
    "        quals = torch.cat((data[\"qual\"], torch.tensor([pad_qual]))).float()\n",
    "        normalized_quals = torch.nn.functional.normalize(quals, dim=0)\n",
    "\n",
    "    # change id to ascii values\n",
    "    rid, target = parse_target(data[\"id\"])\n",
    "\n",
    "    tokenized_inputs.update({\"input_quals\": normalized_quals, \"label\": target})\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels_and_quals_ids(\n",
    "    data, tokenizer, max_length, pad_qual=0, pad_label=IGNORE_INDEX, max_id_length=256\n",
    "):\n",
    "    tokenized_inputs = tokenizer(data[\"seq\"], max_length=max_length, truncation=True, padding=True)\n",
    "    truncation = False\n",
    "\n",
    "    if len(data[\"seq\"]) >= max_length:\n",
    "        truncation = True\n",
    "        quals = torch.cat((data[\"qual\"][: max_length - 1], torch.tensor([pad_qual]))).float()\n",
    "        normalized_quals = torch.nn.functional.normalize(quals, dim=0)\n",
    "    else:\n",
    "        quals = torch.cat((data[\"qual\"], torch.tensor([pad_qual]))).float()\n",
    "        normalized_quals = torch.nn.functional.normalize(quals, dim=0)\n",
    "\n",
    "    # change id to ascii values\n",
    "    rid, target = parse_target(data[\"id\"])\n",
    "\n",
    "    new_id = [len(data[\"id\"]), int(truncation)]\n",
    "    new_id += [ord(char) for char in rid]\n",
    "\n",
    "    if len(new_id) > max_id_length:\n",
    "        new_id = new_id[:max_id_length]\n",
    "    elif len(new_id) < max_id_length:\n",
    "        new_id += [0] * (max_id_length - len(new_id))\n",
    "\n",
    "    tokenized_inputs.update({\"input_quals\": normalized_quals, \"id\": new_id, \"label\": target})\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "def tokenize_dataset(dataset, tokenizer, max_length):\n",
    "    \"\"\"Tokenizes the input dataset using the provided tokenizer and aligns labels and qualities.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): The input dataset to be tokenized.\n",
    "        tokenizer (Tokenizer): The tokenizer to be used for tokenization.\n",
    "        max_length (int): The maximum length of the tokenized sequences.\n",
    "\n",
    "    Returns:\n",
    "        Tokenized dataset with aligned labels and qualities.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the dataset is empty or if the tokenizer is not provided.\n",
    "        TypeError: If the dataset is not of type Dataset or if the tokenizer is not of type Tokenizer.\n",
    "    \"\"\"\n",
    "    if not dataset:\n",
    "        raise ValueError(\"Input dataset is empty\")\n",
    "    if not tokenizer:\n",
    "        raise ValueError(\"Tokenizer is not provided\")\n",
    "    if not isinstance(dataset, Dataset):\n",
    "        raise TypeError(\"Input dataset must be of type Dataset\")\n",
    "\n",
    "    return dataset.map(\n",
    "        partial(tokenize_and_align_labels_and_quals, tokenizer=tokenizer, max_length=max_length)\n",
    "    ).remove_columns([\"id\", \"seq\", \"qual\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "28899bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d0794d3890409987c046c74b71fe68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_train = train_dataset.map(\n",
    "    partial(\n",
    "        tokenize_and_align_labels_and_quals,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=100000,\n",
    "    ),\n",
    "    num_proc=4,  # type: ignore\n",
    ").remove_columns([\"id\", \"seq\", \"qual\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "639a9f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 0,  9,  9,  ...,  7, 10,  1]),\n",
       " 'input_quals': tensor([0.0044, 0.0052, 0.0052,  ..., 0.0022, 0.0022, 0.0000]),\n",
       " 'label': tensor(0)}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "70081d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollator(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2d4e3735",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dataset=data_train,\n",
    "    batch_size=12,\n",
    "    num_workers=1,\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3fbce5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f5dfdea4-a9a9-40a4-859e-81ee94c5b6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "from datasets import Dataset as HuggingFaceDataset\n",
    "from datasets import load_dataset\n",
    "from lightning import LightningDataModule\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "\n",
    "def pad_without_fast_tokenizer_warning(tokenizer, *pad_args, **pad_kwargs):\n",
    "    \"\"\"Pads without triggering the warning about how using the pad function is sub-optimal when using a fast tokenizer.\"\"\"\n",
    "    # To avoid errors when using Feature extractors\n",
    "    if not hasattr(tokenizer, \"deprecation_warnings\"):\n",
    "        return tokenizer.pad(*pad_args, **pad_kwargs)\n",
    "\n",
    "    # Save the state of the warning, then disable it\n",
    "    warning_state = tokenizer.deprecation_warnings.get(\"Asking-to-pad-a-fast-tokenizer\", False)\n",
    "    tokenizer.deprecation_warnings[\"Asking-to-pad-a-fast-tokenizer\"] = True\n",
    "\n",
    "    try:\n",
    "        padded = tokenizer.pad(*pad_args, **pad_kwargs)\n",
    "    finally:\n",
    "        # Restore the state of the warning.\n",
    "        tokenizer.deprecation_warnings[\"Asking-to-pad-a-fast-tokenizer\"] = warning_state\n",
    "\n",
    "    return padded\n",
    "\n",
    "\n",
    "class DataCollator(DataCollatorWithPadding):\n",
    "    def torch_call(self, features):\n",
    "        import torch\n",
    "\n",
    "        label_name = \"label\" if \"label\" in features[0] else \"labels\"\n",
    "        labels = [feature[label_name] for feature in features] if label_name in features[0] else None\n",
    "\n",
    "        qual_name = \"input_quals\"\n",
    "        qual_pad_token_id = 0\n",
    "        input_quals = [feature[qual_name] for feature in features]\n",
    "\n",
    "        id_name = \"id\"  # for predction dataset\n",
    "\n",
    "        no_labels_features = [\n",
    "            {k: v for k, v in feature.items() if k not in [qual_name, label_name, id_name]} for feature in features\n",
    "        ]\n",
    "\n",
    "        batch = pad_without_fast_tokenizer_warning(\n",
    "            self.tokenizer,\n",
    "            no_labels_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        if labels is None:\n",
    "            return batch\n",
    "\n",
    "        sequence_length = batch[\"input_ids\"].shape[1]\n",
    "        padding_side = self.tokenizer.padding_side\n",
    "\n",
    "        def to_list(tensor_or_iterable):\n",
    "            if isinstance(tensor_or_iterable, torch.Tensor):\n",
    "                return tensor_or_iterable.tolist()\n",
    "            return list(tensor_or_iterable)\n",
    "\n",
    "        if padding_side == \"right\":\n",
    "            batch[qual_name] = [\n",
    "                to_list(qual) + [qual_pad_token_id] * (sequence_length - len(qual)) for qual in input_quals\n",
    "            ]\n",
    "        else:\n",
    "            batch[qual_name] = [\n",
    "                [qual_pad_token_id] * (sequence_length - len(qual)) + to_list(qual) for qual in input_quals\n",
    "            ]\n",
    "\n",
    "        batch[qual_name] = torch.tensor(batch[qual_name], dtype=torch.float32)\n",
    "\n",
    "        # for predction dataset and save id feature\n",
    "        if id_name in features[0]:\n",
    "            batch[id_name] = torch.tensor([to_list(feature[id_name]) for feature in features], dtype=torch.int8)\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "class FqDataModule(LightningDataModule):\n",
    "    \"\"\"`LightningDataModule` for the fq dataset.\n",
    "\n",
    "    A `LightningDataModule` implements 7 key methods:\n",
    "\n",
    "    ```python\n",
    "        def prepare_data(self):\n",
    "        # Things to do on 1 GPU/TPU (not on every GPU/TPU in DDP).\n",
    "        # Download data, pre-process, split, save to disk, etc...\n",
    "\n",
    "        def setup(self, stage):\n",
    "        # Things to do on every process in DDP.\n",
    "        # Load data, set variables, etc...\n",
    "\n",
    "        def train_dataloader(self):\n",
    "        # return train dataloader\n",
    "\n",
    "        def val_dataloader(self):\n",
    "        # return validation dataloader\n",
    "\n",
    "        def test_dataloader(self):\n",
    "        # return test dataloader\n",
    "\n",
    "        def predict_dataloader(self):\n",
    "        # return predict dataloader\n",
    "\n",
    "        def teardown(self, stage):\n",
    "        # Called on every process in DDP.\n",
    "        # Clean up after fit or test.\n",
    "    ```\n",
    "\n",
    "    This allows you to share a full dataset without explaining how to download,\n",
    "    split, transform and process the data.\n",
    "\n",
    "    Read the docs:\n",
    "        https://lightning.ai/docs/pytorch/latest/data/datamodule.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        train_data_path: Path,\n",
    "        val_data_path: Path | None = None,\n",
    "        test_data_path: Path | None = None,\n",
    "        predict_data_path: Path | None = None,\n",
    "        train_val_test_split: tuple[float, float, float] = (0.7, 0.2, 0.1),\n",
    "        batch_size: int = 12,\n",
    "        num_workers: int = 0,\n",
    "        max_train_samples: int | None = None,\n",
    "        max_val_samples: int | None = None,\n",
    "        max_test_samples: int | None = None,\n",
    "        max_predict_samples: int | None = None,\n",
    "        *,\n",
    "        pin_memory: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize a `FqDataModule`.\n",
    "\n",
    "        :param train_val_test_split: The train, validation and test split. Defaults to `(55_000, 5_000, 10_000)`.\n",
    "        :param batch_size: The batch size. Defaults to `64`.\n",
    "        :param num_workers: The number of workers. Defaults to `0`.\n",
    "        :param pin_memory: Whether to pin memory. Defaults to `False`.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # this line allows to access init params with 'self.hparams' attribute\n",
    "        # also ensures init params will be stored in ckpt\n",
    "        self.save_hyperparameters(logger=False)\n",
    "\n",
    "        self.data_train: Dataset | None = None\n",
    "        self.data_val: Dataset | None = None\n",
    "        self.data_test: Dataset | None = None\n",
    "        self.batch_size_per_device = batch_size\n",
    "        self.data_collator = DataCollator(tokenizer)\n",
    "\n",
    "    @property\n",
    "    def num_classes(self) -> int:\n",
    "        \"\"\"Get the number of classes.\"\"\"\n",
    "        return 2\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        \"\"\"Encode the FastQ data to Parquet format.\"\"\"\n",
    "        data_paths = [self.hparams.train_data_path]\n",
    "\n",
    "        if self.hparams.val_data_path is not None:\n",
    "            data_paths.append(self.hparams.val_data_path)\n",
    "\n",
    "        if self.hparams.test_data_path is not None:\n",
    "            data_paths.append(self.hparams.test_data_path)\n",
    "\n",
    "        if self.hparams.predict_data_path is not None:\n",
    "            data_paths.append(self.hparams.predict_data_path)\n",
    "\n",
    "        for data_path in data_paths:\n",
    "            if Path(data_path).suffix == \".parquet\":\n",
    "                pass\n",
    "            else:\n",
    "                msg = f\"Data file {data_path} is not in Parquet format.\"\n",
    "                raise ValueError(msg)\n",
    "\n",
    "        self.hparams.train_data_path = Path(self.hparams.train_data_path).with_suffix(\".parquet\").as_posix()\n",
    "\n",
    "        if self.hparams.val_data_path is not None:\n",
    "            self.hparams.val_data_path = Path(self.hparams.val_data_path).with_suffix(\".parquet\").as_posix()\n",
    "\n",
    "        if self.hparams.test_data_path is not None:\n",
    "            self.hparams.test_data_path = Path(self.hparams.test_data_path).with_suffix(\".parquet\").as_posix()\n",
    "\n",
    "        if self.hparams.predict_data_path is not None:\n",
    "            self.hparams.predict_data_path = Path(self.hparams.predict_data_path).with_suffix(\".parquet\").as_posix()\n",
    "\n",
    "    def setup(self, stage: str | None = None) -> None:\n",
    "        \"\"\"Load data. Set variables: `self.data_train`, `self.data_val`, `self.data_test`.\n",
    "\n",
    "        This method is called by Lightning before `trainer.fit()`, `trainer.validate()`, `trainer.test()`, and\n",
    "        `trainer.predict()`, so be careful not to execute things like random split twice! Also, it is called after\n",
    "        `self.prepare_data()` and there is a barrier in between which ensures that all the processes proceed to\n",
    "        `self.setup()` once the data is prepared and available for use.\n",
    "\n",
    "        :param stage: The stage to setup. Either `\"fit\"`, `\"validate\"`, `\"test\"`, or `\"predict\"`. Defaults to ``None``.\n",
    "        \"\"\"\n",
    "        # Divide batch size by the number of devices.\n",
    "        if self.trainer is not None:\n",
    "            if self.hparams.batch_size % self.trainer.world_size != 0:\n",
    "                msg = f\"Batch size ({self.hparams.batch_size}) is not divisible by the number of devices ({self.trainer.world_size}).\"\n",
    "                raise RuntimeError(msg)\n",
    "            self.batch_size_per_device = self.hparams.batch_size // self.trainer.world_size\n",
    "\n",
    "        if stage == \"predict\":\n",
    "            if not self.hparams.predict_data_path:\n",
    "                msg = \"Predict data path is required for prediction stage.\"\n",
    "                raise ValueError(msg)\n",
    "\n",
    "            num_proc = min(self.hparams.num_workers, multiprocessing.cpu_count() - 1)\n",
    "            data_files = {\"predict\": self.hparams.predict_data_path}\n",
    "            predict_dataset = load_dataset(\n",
    "                \"parquet\",\n",
    "                data_files=data_files,\n",
    "                num_proc=max(1, num_proc),\n",
    "            ).with_format(\"torch\")\n",
    "\n",
    "            predict_dataset = predict_dataset[\"predict\"]\n",
    "            if self.hparams.max_predict_samples is not None:\n",
    "                max_predict_samples = min(self.hparams.max_predict_samples, len(predict_dataset))\n",
    "                predict_dataset = HuggingFaceDataset.from_dict(predict_dataset[:max_predict_samples]).with_format(\n",
    "                    \"torch\"\n",
    "                )\n",
    "\n",
    "            self.data_predict = predict_dataset.map(\n",
    "                partial(\n",
    "                    tokenize_and_align_labels_and_quals_ids,\n",
    "                    tokenizer=self.hparams.tokenizer,\n",
    "                    max_length=self.hparams.tokenizer.max_len_single_sentence,\n",
    "                ),\n",
    "                num_proc=max(1, num_proc),  # type: ignore\n",
    "            ).remove_columns([\"seq\", \"qual\", \"target\"])\n",
    "            del predict_dataset\n",
    "            return\n",
    "\n",
    "        # load and split datasets only if not loaded already\n",
    "        if not self.data_train and not self.data_val and not self.data_test:\n",
    "            num_proc = min(self.hparams.num_workers, multiprocessing.cpu_count() - 1)\n",
    "            data_files = {}\n",
    "            data_files[\"train\"] = self.hparams.train_data_path\n",
    "\n",
    "            if self.hparams.val_data_path is not None:\n",
    "                data_files[\"validation\"] = self.hparams.val_data_path\n",
    "\n",
    "            if self.hparams.test_data_path is not None:\n",
    "                data_files[\"test\"] = self.hparams.test_data_path\n",
    "\n",
    "            if self.hparams.val_data_path is None or self.hparams.test_data_path is None:\n",
    "                split_percent = self.hparams.train_val_test_split\n",
    "\n",
    "                train_dataset = load_dataset(\n",
    "                    \"parquet\",\n",
    "                    data_files=data_files,\n",
    "                    num_proc=max(1, num_proc),\n",
    "                    split=f\"train[:{split_percent[0]}%]\",\n",
    "                ).with_format(\"torch\")\n",
    "\n",
    "                val_dataset = load_dataset(\n",
    "                    \"parquet\",\n",
    "                    data_files=data_files,\n",
    "                    num_proc=max(1, num_proc),\n",
    "                    split=f\"train[{split_percent[0]}%:{split_percent[0] + split_percent[1]}%]\",\n",
    "                ).with_format(\"torch\")\n",
    "\n",
    "                test_dataset = load_dataset(\n",
    "                    \"parquet\",\n",
    "                    data_files=data_files,\n",
    "                    num_proc=max(1, num_proc),\n",
    "                    split=f\"train[{split_percent[0] + split_percent[1]}%:]\",\n",
    "                ).with_format(\"torch\")\n",
    "\n",
    "            else:\n",
    "                raw_datasets = load_dataset(\"parquet\", data_files=data_files, num_proc=max(1, num_proc)).with_format(\n",
    "                    \"torch\"\n",
    "                )\n",
    "\n",
    "                train_dataset = raw_datasets[\"train\"]\n",
    "                val_dataset = raw_datasets[\"validation\"]\n",
    "                test_dataset = raw_datasets[\"test\"]\n",
    "\n",
    "            if self.hparams.max_train_samples is not None:\n",
    "                max_train_samples = min(self.hparams.max_train_samples, len(train_dataset))\n",
    "                train_dataset = HuggingFaceDataset.from_dict(train_dataset[:max_train_samples]).with_format(\"torch\")\n",
    "\n",
    "            if self.hparams.max_val_samples is not None:\n",
    "                max_val_samples = min(self.hparams.max_val_samples, len(val_dataset))\n",
    "                val_dataset = HuggingFaceDataset.from_dict(val_dataset[:max_val_samples]).with_format(\"torch\")\n",
    "\n",
    "            if self.hparams.max_test_samples is not None:\n",
    "                max_test_samples = min(self.hparams.max_test_samples, len(test_dataset))\n",
    "                test_dataset = HuggingFaceDataset.from_dict(test_dataset[:max_test_samples]).with_format(\"torch\")\n",
    "\n",
    "            self.data_train = train_dataset.map(\n",
    "                partial(\n",
    "                    tokenize_and_align_labels_and_quals,\n",
    "                    tokenizer=self.hparams.tokenizer,\n",
    "                    max_length=self.hparams.tokenizer.max_len_single_sentence,\n",
    "                ),\n",
    "                num_proc=max(1, num_proc),  # type: ignore\n",
    "            ).remove_columns([\"id\", \"seq\", \"qual\"])\n",
    "\n",
    "            self.data_val = val_dataset.map(\n",
    "                partial(\n",
    "                    tokenize_and_align_labels_and_quals,\n",
    "                    tokenizer=self.hparams.tokenizer,\n",
    "                    max_length=self.hparams.tokenizer.max_len_single_sentence,\n",
    "                ),\n",
    "                num_proc=max(1, num_proc),  # type: ignore\n",
    "            ).remove_columns([\"id\", \"seq\", \"qual\"])\n",
    "\n",
    "            self.data_test = test_dataset.map(\n",
    "                partial(\n",
    "                    tokenize_and_align_labels_and_quals,\n",
    "                    tokenizer=self.hparams.tokenizer,\n",
    "                    max_length=self.hparams.tokenizer.max_len_single_sentence,\n",
    "                ),\n",
    "                num_proc=max(1, num_proc),  # type: ignore\n",
    "            ).remove_columns([\"id\", \"seq\", \"qual\"])\n",
    "\n",
    "            del train_dataset, val_dataset, test_dataset\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader[Any]:\n",
    "        \"\"\"Create and return the train dataloader.\n",
    "\n",
    "        :return: The train dataloader.\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            dataset=self.data_train,\n",
    "            batch_size=self.batch_size_per_device,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=self.hparams.pin_memory,\n",
    "            collate_fn=self.data_collator.torch_call,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader[Any]:\n",
    "        \"\"\"Create and return the validation dataloader.\n",
    "\n",
    "        :return: The validation dataloader.\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            dataset=self.data_val,\n",
    "            batch_size=self.batch_size_per_device,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=self.hparams.pin_memory,\n",
    "            collate_fn=self.data_collator.torch_call,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader[Any]:\n",
    "        \"\"\"Create and return the test dataloader.\n",
    "\n",
    "        :return: The test dataloader.\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            dataset=self.data_test,\n",
    "            batch_size=self.batch_size_per_device,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=self.hparams.pin_memory,\n",
    "            collate_fn=self.data_collator.torch_call,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self) -> DataLoader[Any]:\n",
    "        \"\"\"Create and return the predict dataloader.\n",
    "\n",
    "        :return: The predict dataloader.\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            dataset=self.data_predict,\n",
    "            batch_size=self.batch_size_per_device,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=self.hparams.pin_memory,\n",
    "            collate_fn=self.data_collator.torch_call,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "    def teardown(self, stage: str | None = None) -> None:\n",
    "        \"\"\"Lightning hook for cleaning up after `trainer.fit()`, `trainer.validate()`,.\n",
    "\n",
    "        `trainer.test()`, and `trainer.predict()`.\n",
    "\n",
    "        :param stage: The stage being torn down. Either `\"fit\"`, `\"validate\"`, `\"test\"`, or `\"predict\"`.\n",
    "            Defaults to ``None``.\n",
    "        \"\"\"\n",
    "\n",
    "    def state_dict(self) -> dict[Any, Any]:\n",
    "        \"\"\"Called when saving a checkpoint. Implement to generate and save the datamodule state.\n",
    "\n",
    "        :return: A dictionary containing the datamodule state that you want to save.\n",
    "        \"\"\"\n",
    "        return {}\n",
    "\n",
    "    def load_state_dict(self, state_dict: dict[str, Any]) -> None:\n",
    "        \"\"\"Called when loading a checkpoint. Implement to reload datamodule state given datamodule.\n",
    "\n",
    "        `state_dict()`.\n",
    "\n",
    "        :param state_dict: The datamodule state returned by `self.state_dict()`.\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2999f3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fq_data_module = FqDataModule(tokenizer, dataset_folder / \"test.fq.target.fq.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d23684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1444c9b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "902d02c2fd76480db3dea90951a4bdea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe1a58ffc3694b168d8b37e310974c8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e5a432cdf34095a1a7671586eac8f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import chimera\n",
    "\n",
    "train_data_path = \"../data/train_data/80000/test.fq.target.fq.parquet\"\n",
    "max_len = 100000\n",
    "# Load the training data\n",
    "tokenizer = chimera.data.tokenizer.Tokenizer(model_max_length=max_len)\n",
    "fq_data_module = chimera.data.fq.DataModule(tokenizer, train_data_path)\n",
    "fq_data_module.prepare_data()\n",
    "fq_data_module.setup()\n",
    "train_data_loader = fq_data_module.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "769f7860",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iterator = iter(train_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "00c97656",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = next(data_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "079d4eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 8265])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "34bf6629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 8265])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1[\"input_quals\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8fe91b83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9f6c2b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resume_read_name(bytes_data: torch.Tensor | list[int]):\n",
    "    # Convert bytes to string\n",
    "    if isinstance(bytes_data, torch.Tensor):\n",
    "        if bytes_data.numel() == 0:\n",
    "            return \"\"\n",
    "        bytes_data = bytes_data.tolist()\n",
    "    elif not bytes_data:\n",
    "        return \"\"\n",
    "        \n",
    "    try:\n",
    "        read_name_length = bytes_data[0]\n",
    "        if read_name_length <= 0 or read_name_length >= len(bytes_data):\n",
    "            return \"\"\n",
    "        read_name = bytes_data[1:1+read_name_length] \n",
    "        return \"\".join(chr(b) for b in read_name if 32 <= b <= 126)  # Only valid ASCII printable chars\n",
    "    except (IndexError, TypeError):\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "16c6d867",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Predict:\n",
    "    name: str\n",
    "    label: int\n",
    "    sv: str | None = None\n",
    "\n",
    "def collect_predict_from_file(path):\n",
    "    predicts = torch.load(path, weights_only=True)\n",
    "    read_names = [resume_read_name(id) for id in predicts[\"id\"]]\n",
    "    labels = predicts[\"prediction\"].argmax(dim=1).tolist()\n",
    "    return (Predict(name, label) for name, label in zip(read_names, labels))\n",
    "\n",
    "def collect_predict_from_folder(folder: Path| str):\n",
    "    if isinstance(folder, str):\n",
    "        folder = Path(folder)\n",
    "\n",
    "    for file in folder.glob(\"*.pt\"):\n",
    "        yield from collect_predict_from_file(file)\n",
    "\n",
    "def summarize_predict(predicts):\n",
    "    total = 0\n",
    "    number_label_1 = 0\n",
    "    for predict in predicts:\n",
    "        total += 1\n",
    "        if predict.label == 1:\n",
    "            number_label_1 += 1\n",
    "    return total, number_label_1\n",
    "\n",
    "def write_predicts(predicts, path):\n",
    "    total = 0 \n",
    "    number_label_1 = 0\n",
    "\n",
    "    with open(path, \"w\") as f:\n",
    "        for predict in predicts:\n",
    "            total += 1\n",
    "            if predict.label == 1:\n",
    "                number_label_1 += 1\n",
    "            f.write(f\"{predict.name}\\t{predict.label}\\n\")\n",
    "    \n",
    "    return total, number_label_1\n",
    "\n",
    "\n",
    "def load_predicts(path) -> list[Predict]:\n",
    "    with open(path) as f:\n",
    "        predicts = []\n",
    "        for line in f:\n",
    "            name, label = line.strip().split(\"\\t\")\n",
    "            predicts.append(Predict(name, int(label)))\n",
    "        return predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38dbdc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c857fe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_path = Path(\"/projects/b1171/ylk4626/project/Chimera/logs/eval/runs/2025-02-20_19-01-17\")\n",
    "# /projects/b1171/ylk4626/project/Chimera/logs/eval/runs/2025-02-20_20-30-32\n",
    "# /projects/b1171/ylk4626/project/Chimera/logs/eval/runs/2025-02-20_22-06-13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "551860da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the memory usage\n",
    "pt = collect_predict_from_folder(predict_path / \"predicts/0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "da10241e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "write_predicts(pt, predict_path / \"predicts/predicts.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "686dc895-eca2-434d-8e4a-88d4bc9630b2",
   "metadata": {},
   "outputs": [],
   "source": [
    " # data/sv/PC3_10_cells_MDA_Mk1c_dirty/chimeric_reads_mapping/cutesv.vcf.sv.read.sup.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63ca7d2-8849-4435-aae0-051097828ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dc",
   "language": "python",
   "name": "dc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
